{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import numpy as np\n",
    "\n",
    "# Assuming data_and_output_dir is defined and contains the path to your data directory\n",
    "data_and_output_dir = '/Users/ansonkong/Downloads/Data for nyu work/'  # Update this path\n",
    "\n",
    "# Path to the shapefile and CSV\n",
    "shapefile_path = data_and_output_dir + 'Input/cb_2018_us_county_500k/cb_2018_us_county_500k.shp'\n",
    "csv_path = data_and_output_dir + 'Input/ba_service_territory_2020.csv'\n",
    "mainland_fips_ranges = [\n",
    "    (1001, 1999),  # Alabama\n",
    "    (4001, 4999),  # Arizona\n",
    "    (5001, 5999),  # Arkansas\n",
    "    (6001, 6999),  # California\n",
    "    (8001, 8999),  # Colorado\n",
    "    (9001, 9999),  # Connecticut\n",
    "    (10001, 10999),  # Delaware\n",
    "    (11001, 11999),  # District of Columbia\n",
    "    (12001, 12999),  # Florida\n",
    "    (13001, 13999),  # Georgia\n",
    "    (16001, 16999),  # Idaho\n",
    "    (17001, 17999),  # Illinois\n",
    "    (18001, 18999),  # Indiana\n",
    "    (19001, 19999),  # Iowa\n",
    "    (20001, 20999),  # Kansas\n",
    "    (21001, 21999),  # Kentucky\n",
    "    (22001, 22999),  # Louisiana\n",
    "    (23001, 23999),  # Maine\n",
    "    (24001, 24999),  # Maryland\n",
    "    (25001, 25999),  # Massachusetts\n",
    "    (26001, 26999),  # Michigan\n",
    "    (27001, 27999),  # Minnesota\n",
    "    (28001, 28999),  # Mississippi\n",
    "    (29001, 29999),  # Missouri\n",
    "    (30001, 30999),  # Montana\n",
    "    (31001, 31999),  # Nebraska\n",
    "    (32001, 32999),  # Nevada\n",
    "    (33001, 33999),  # New Hampshire\n",
    "    (34001, 34999),  # New Jersey\n",
    "    (35001, 35999),  # New Mexico\n",
    "    (36001, 36999),  # New York\n",
    "    (37001, 37999),  # North Carolina\n",
    "    (38001, 38999),  # North Dakota\n",
    "    (39001, 39999),  # Ohio\n",
    "    (40001, 40999),  # Oklahoma\n",
    "    (41001, 41999),  # Oregon\n",
    "    (42001, 42999),  # Pennsylvania\n",
    "    (44001, 44999),  # Rhode Island\n",
    "    (45001, 45999),  # South Carolina\n",
    "    (46001, 46999),  # South Dakota\n",
    "    (47001, 47999),  # Tennessee\n",
    "    (48001, 48999),  # Texas\n",
    "    (49001, 49999),  # Utah\n",
    "    (50001, 50999),  # Vermont\n",
    "    (51001, 51999),  # Virginia\n",
    "    (53001, 53999),  # Washington\n",
    "    (54001, 54999),  # West Virginia\n",
    "    (55001, 55999),  # Wisconsin\n",
    "    (56001, 56999)  # Wyoming\n",
    "]\n",
    "\n",
    "# Assuming the rest of the script remains the same...\n",
    "\n",
    "# Adjust the function to work with strings\n",
    "def is_in_mainland_ranges(geoid_str, ranges):\n",
    "    geoid = int(geoid_str)  # Convert to integer for comparison\n",
    "    for start, end in ranges:\n",
    "        if start <= geoid <= end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Read the shapefile using GeoPandas\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert County_FIPS to string and ensure it's zero-padded to 5 digits\n",
    "df['GEOID'] = df['County_FIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "# Filter df to include only rows with GEOID in the specified mainland FIPS ranges\n",
    "# Note: Now 'GEOID' in df is already a string, so we keep it consistent.\n",
    "df = df[df['GEOID'].apply(lambda geoid: is_in_mainland_ranges(geoid, mainland_fips_ranges))]\n",
    "\n",
    "# Ensure 'GEOID' in gdf is also a string to match types during merge\n",
    "gdf['GEOID'] = gdf['GEOID'].astype(str)\n",
    "\n",
    "# Convert GEOID in both gdf and df to string for consistency\n",
    "gdf['GEOID'] = gdf['GEOID'].astype(str)\n",
    "df['GEOID'] = df['GEOID'].astype(str)\n",
    "\n",
    "# Now proceed to merge - both GEOID columns are strings\n",
    "merged_gdf = gdf.merge(df, on='GEOID')\n",
    "\n",
    "\n",
    "\n",
    "# Check and fix invalid geometries if necessary\n",
    "invalid_geometries = merged_gdf[~merged_gdf.is_valid]\n",
    "if invalid_geometries.shape[0] > 0:\n",
    "    merged_gdf = merged_gdf.buffer(0)\n",
    "\n",
    "# Aggregate (dissolve) geometries by BA_Code to get a single geometry per BA_Code\n",
    "ba_areas = merged_gdf.dissolve(by='BA_Code', as_index=False)\n",
    "\n",
    "\n",
    "# Convert the CRS of ba_areas to EPSG:3857 for contextily\n",
    "ba_areas = ba_areas.to_crs(epsg=3857)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "\n",
    "# Use a more diverse colormap, for example, 'tab20', 'Set3', or define your own\n",
    "# 'tab20' is good for up to 20 distinct categories; for more categories, consider other options or custom colormaps\n",
    "ba_areas.plot(ax=ax, column='BA_Code', edgecolor='black', linewidth=1, legend=True, cmap='tab20', legend_kwds={'bbox_to_anchor': (1, 1)})\n",
    "\n",
    "# Add basemap\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "# Plot each BA_Code area with label - using representative_point for label placement\n",
    "for idx, row in ba_areas.iterrows():\n",
    "    # Use representative_point() for potentially better label placement\n",
    "    rep_point = row.geometry.representative_point()\n",
    "    plt.text(rep_point.x, rep_point.y, row['BA_Code'], fontsize=8, ha='center', va='center', color='white')\n",
    "\n",
    "# Adjust the axis to the bounds of your geodataframe if valid\n",
    "if not np.any(np.isnan(ba_areas.total_bounds)) and not np.any(np.isinf(ba_areas.total_bounds)):\n",
    "    ax.set_xlim(ba_areas.total_bounds[[0, 2]])\n",
    "    ax.set_ylim(ba_areas.total_bounds[[1, 3]])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from colorsys import hls_to_rgb\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "\n",
    "control_area_path = data_and_output_dir + 'input/Electric_Retail_Service_Territories/Electric_Retail_Service_Territories.shp'\n",
    "nc_file_path = data_and_output_dir + 'input/tgw_wrf_rcp85hotter_hourly_2060-01-01_01_00_00.nc'\n",
    "base_file_path = data_and_output_dir + 'input/US_CAN_MEX_PCA_polygons.csv'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "control_areas_gdf = gpd.read_file(control_area_path).to_crs(epsg=4326)\n",
    "\n",
    "base_df = pd.read_csv(base_file_path)\n",
    "# Assuming 'WKT' column exists and contains geometries\n",
    "base_df['geometry'] = base_df['WKT'].apply(wkt.loads)\n",
    "base_gdf = gpd.GeoDataFrame(base_df, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "ds = nc.Dataset(nc_file_path, 'r')\n",
    "time_step = 0  # Adjust based on your needs\n",
    "lats = ds.variables['XLAT'][time_step, :, :]\n",
    "lons = ds.variables['XLONG'][time_step, :, :]\n",
    "points_geometry = [Point(lon, lat) for lat, lon in zip(np.ravel(lats), np.ravel(lons))]\n",
    "gdf_points = gpd.GeoDataFrame(geometry=points_geometry, crs='EPSG:4326')\n",
    "\n",
    "# Assuming 'rb' is a column in base_gdf\n",
    "gdf_points_with_rb = gpd.sjoin(gdf_points, base_gdf[['geometry', 'rb']], how='inner', op='intersects')\n",
    "\n",
    "# Drop 'index_left' and 'index_right' columns if they exist\n",
    "if 'index_left' in gdf_points_with_rb.columns:\n",
    "    gdf_points_with_rb.drop(columns=['index_left'], inplace=True)\n",
    "if 'index_right' in gdf_points_with_rb.columns:\n",
    "    gdf_points_with_rb.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "# Now perform the spatial join again\n",
    "gdf_points_with_control_area = gpd.sjoin(gdf_points_with_rb, control_areas_gdf, how='inner', op='intersects')\n",
    "\n",
    "# Group by 'rb' to determine the majority control area\n",
    "majority_control_area_by_rb = gdf_points_with_control_area.groupby('rb')['CNTRL_AREA'].agg(lambda x: x.mode()[0] if not x.empty else 'No Control Area').reset_index()\n",
    "\n",
    "# Use 'CNTRL_AREA' from control_areas_gdf for grouping\n",
    "\n",
    "for index, row in base_gdf.iterrows():\n",
    "    if row['rb'] not in majority_control_area_by_rb['rb'].values:\n",
    "        # Check if RB is fully within a control area\n",
    "        contained_areas = control_areas_gdf[control_areas_gdf.contains(row['geometry'])]['CNTRL_AREA'].unique()\n",
    "        if len(contained_areas) == 1:\n",
    "            # Add directly to majority_control_area_by_rb DataFrame\n",
    "            new_row = {'rb': row['rb'], 'CNTRL_AREA': contained_areas[0]}\n",
    "            majority_control_area_by_rb = majority_control_area_by_rb.append(new_row, ignore_index=True)\n",
    "# Assuming base_df is already loaded\n",
    "base_df['geometry'] = base_df['WKT'].apply(wkt.loads)\n",
    "base_gdf = gpd.GeoDataFrame(base_df, geometry='geometry', crs='EPSG:4326')\n",
    "# Ensure the 'rb' column is the same type in both DataFrames\n",
    "base_gdf['rb'] = base_gdf['rb'].astype(str)\n",
    "majority_control_area_by_rb['rb'] = majority_control_area_by_rb['rb'].astype(str)\n",
    "\n",
    "# Merge to associate each RB geometry with its majority control area\n",
    "rb_with_control_area = base_gdf[['rb', 'geometry']].merge(majority_control_area_by_rb, on='rb')\n",
    "# Filter RBs from p1 to p134\n",
    "rb_filtered = rb_with_control_area[rb_with_control_area['rb'].isin([f'p{i}' for i in range(1, 135)])]\n",
    "\n",
    "# Convert to Web Mercator\n",
    "rb_filtered_crs = rb_filtered.to_crs(epsg=3857)\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate a wide-range colormap\n",
    "def generate_colormap(n_colors):\n",
    "    colors = []\n",
    "    for i in np.linspace(0, 1, n_colors):\n",
    "        h = i  # Hue\n",
    "        l = 0.5  # Lightness\n",
    "        s = 0.8  # Saturation\n",
    "        rgb = hls_to_rgb(h, l, s)\n",
    "        colors.append(rgb)\n",
    "    return LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=n_colors)\n",
    "\n",
    "# Assuming rb_filtered_crs is your GeoDataFrame ready for plotting\n",
    "num_unique_areas = rb_filtered_crs['CNTRL_AREA'].nunique()\n",
    "custom_cmap = generate_colormap(num_unique_areas)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot using the generated custom colormap\n",
    "rb_filtered_crs.plot(ax=ax, column='CNTRL_AREA', legend=True, cmap=custom_cmap,\n",
    "                     edgecolor='black', linewidth=1,\n",
    "                     legend_kwds={'loc': 'upper center', 'bbox_to_anchor': (0.5, -0.15), 'ncol': 3, 'title': 'Control Area'})\n",
    "\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=rb_filtered_crs.crs.to_string())\n",
    "ax.set_title('RBs within Control Area')\n",
    "\n",
    "for idx, row in rb_filtered_crs.iterrows():\n",
    "    centroid = row.geometry.centroid.coords[0]\n",
    "    plt.text(centroid[0], centroid[1], row['rb'], fontsize=8, ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from colorsys import hls_to_rgb\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "import numpy as np\n",
    "\n",
    "# Assuming data_and_output_dir is defined and contains the path to your data directory\n",
    "data_and_output_dir = '/Users/ansonkong/Downloads/Data for nyu work/'  # Update this path\n",
    "\n",
    "# Path to the shapefile and CSV\n",
    "shapefile_path = data_and_output_dir + 'Input/cb_2018_us_county_500k/cb_2018_us_county_500k.shp'\n",
    "csv_path = data_and_output_dir + 'Input/ba_service_territory_2020.csv'\n",
    "mainland_fips_ranges = [\n",
    "    (1001, 1999),  # Alabama\n",
    "    (4001, 4999),  # Arizona\n",
    "    (5001, 5999),  # Arkansas\n",
    "    (6001, 6999),  # California\n",
    "    (8001, 8999),  # Colorado\n",
    "    (9001, 9999),  # Connecticut\n",
    "    (10001, 10999),  # Delaware\n",
    "    (11001, 11999),  # District of Columbia\n",
    "    (12001, 12999),  # Florida\n",
    "    (13001, 13999),  # Georgia\n",
    "    (16001, 16999),  # Idaho\n",
    "    (17001, 17999),  # Illinois\n",
    "    (18001, 18999),  # Indiana\n",
    "    (19001, 19999),  # Iowa\n",
    "    (20001, 20999),  # Kansas\n",
    "    (21001, 21999),  # Kentucky\n",
    "    (22001, 22999),  # Louisiana\n",
    "    (23001, 23999),  # Maine\n",
    "    (24001, 24999),  # Maryland\n",
    "    (25001, 25999),  # Massachusetts\n",
    "    (26001, 26999),  # Michigan\n",
    "    (27001, 27999),  # Minnesota\n",
    "    (28001, 28999),  # Mississippi\n",
    "    (29001, 29999),  # Missouri\n",
    "    (30001, 30999),  # Montana\n",
    "    (31001, 31999),  # Nebraska\n",
    "    (32001, 32999),  # Nevada\n",
    "    (33001, 33999),  # New Hampshire\n",
    "    (34001, 34999),  # New Jersey\n",
    "    (35001, 35999),  # New Mexico\n",
    "    (36001, 36999),  # New York\n",
    "    (37001, 37999),  # North Carolina\n",
    "    (38001, 38999),  # North Dakota\n",
    "    (39001, 39999),  # Ohio\n",
    "    (40001, 40999),  # Oklahoma\n",
    "    (41001, 41999),  # Oregon\n",
    "    (42001, 42999),  # Pennsylvania\n",
    "    (44001, 44999),  # Rhode Island\n",
    "    (45001, 45999),  # South Carolina\n",
    "    (46001, 46999),  # South Dakota\n",
    "    (47001, 47999),  # Tennessee\n",
    "    (48001, 48999),  # Texas\n",
    "    (49001, 49999),  # Utah\n",
    "    (50001, 50999),  # Vermont\n",
    "    (51001, 51999),  # Virginia\n",
    "    (53001, 53999),  # Washington\n",
    "    (54001, 54999),  # West Virginia\n",
    "    (55001, 55999),  # Wisconsin\n",
    "    (56001, 56999)  # Wyoming\n",
    "]\n",
    "\n",
    "# Assuming the rest of the script remains the same...\n",
    "\n",
    "# Adjust the function to work with strings\n",
    "def is_in_mainland_ranges(geoid_str, ranges):\n",
    "    geoid = int(geoid_str)  # Convert to integer for comparison\n",
    "    for start, end in ranges:\n",
    "        if start <= geoid <= end:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Read the shapefile using GeoPandas\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert County_FIPS to string and ensure it's zero-padded to 5 digits\n",
    "df['GEOID'] = df['County_FIPS'].apply(lambda x: str(x).zfill(5))\n",
    "\n",
    "# Filter df to include only rows with GEOID in the specified mainland FIPS ranges\n",
    "# Note: Now 'GEOID' in df is already a string, so we keep it consistent.\n",
    "df = df[df['GEOID'].apply(lambda geoid: is_in_mainland_ranges(geoid, mainland_fips_ranges))]\n",
    "\n",
    "# Ensure 'GEOID' in gdf is also a string to match types during merge\n",
    "gdf['GEOID'] = gdf['GEOID'].astype(str)\n",
    "\n",
    "# Convert GEOID in both gdf and df to string for consistency\n",
    "gdf['GEOID'] = gdf['GEOID'].astype(str)\n",
    "df['GEOID'] = df['GEOID'].astype(str)\n",
    "\n",
    "# Now proceed to merge - both GEOID columns are strings\n",
    "merged_gdf = gdf.merge(df, on='GEOID')\n",
    "\n",
    "\n",
    "\n",
    "# Check and fix invalid geometries if necessary\n",
    "invalid_geometries = merged_gdf[~merged_gdf.is_valid]\n",
    "if invalid_geometries.shape[0] > 0:\n",
    "    merged_gdf = merged_gdf.buffer(0)\n",
    "\n",
    "# Aggregate (dissolve) geometries by BA_Code to get a single geometry per BA_Code\n",
    "ba_areas = merged_gdf.dissolve(by='BA_Code', as_index=False)\n",
    "\n",
    "\n",
    "# Convert the CRS of ba_areas to EPSG:4326 for contextily\n",
    "ba_areas = ba_areas.to_crs(epsg=4326)\n",
    "\n",
    "nc_file_path = data_and_output_dir + 'input/tgw_wrf_rcp85hotter_hourly_2060-01-01_01_00_00.nc'\n",
    "base_file_path = data_and_output_dir + 'input/US_CAN_MEX_PCA_polygons.csv'\n",
    "\n",
    "base_df = pd.read_csv(base_file_path)\n",
    "# Assuming 'WKT' column exists and contains geometries\n",
    "base_df['geometry'] = base_df['WKT'].apply(wkt.loads)\n",
    "base_gdf = gpd.GeoDataFrame(base_df, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "ds = nc.Dataset(nc_file_path, 'r')\n",
    "time_step = 0  # Adjust based on your needs\n",
    "lats = ds.variables['XLAT'][time_step, :, :]\n",
    "lons = ds.variables['XLONG'][time_step, :, :]\n",
    "points_geometry = [Point(lon, lat) for lat, lon in zip(np.ravel(lats), np.ravel(lons))]\n",
    "gdf_points = gpd.GeoDataFrame(geometry=points_geometry, crs='EPSG:4326')\n",
    "\n",
    "# Assuming 'rb' is a column in base_gdf\n",
    "gdf_points_with_rb = gpd.sjoin(gdf_points, base_gdf[['geometry', 'rb']], how='inner', op='intersects')\n",
    "\n",
    "# Drop 'index_left' and 'index_right' columns if they exist\n",
    "if 'index_left' in gdf_points_with_rb.columns:\n",
    "    gdf_points_with_rb.drop(columns=['index_left'], inplace=True)\n",
    "if 'index_right' in gdf_points_with_rb.columns:\n",
    "    gdf_points_with_rb.drop(columns=['index_right'], inplace=True)\n",
    "\n",
    "# Now perform the spatial join again\n",
    "gdf_points_with_ba = gpd.sjoin(gdf_points_with_rb, ba_areas, how='inner', op='intersects')\n",
    "\n",
    "# Group by 'rb' to determine the majority control area\n",
    "majority_ba_by_rb = gdf_points_with_ba.groupby('rb')['BA_Code'].agg(lambda x: x.mode()[0] if not x.empty else 'No BA').reset_index()\n",
    "\n",
    "# Use 'CNTRL_AREA' from control_areas_gdf for grouping\n",
    "\n",
    "for index, row in base_gdf.iterrows():\n",
    "    if row['rb'] not in majority_ba_by_rb['rb'].values:\n",
    "        # Check if RB is fully within a control area\n",
    "        contained_areas = ba_areas[ba_areas.contains(row['geometry'])]['BA_Code'].unique()\n",
    "        if len(contained_areas) == 1:\n",
    "            # Add directly to majority_control_area_by_rb DataFrame\n",
    "            new_row = {'rb': row['rb'], 'BA_Code': contained_areas[0]}\n",
    "            majority_ba_by_rb = majority_ba_by_rb.append(new_row, ignore_index=True)\n",
    "# Assuming base_df is already loaded\n",
    "base_df['geometry'] = base_df['WKT'].apply(wkt.loads)\n",
    "base_gdf = gpd.GeoDataFrame(base_df, geometry='geometry', crs='EPSG:4326')\n",
    "# Ensure the 'rb' column is the same type in both DataFrames\n",
    "base_gdf['rb'] = base_gdf['rb'].astype(str)\n",
    "majority_ba_by_rb['rb'] = majority_ba_by_rb['rb'].astype(str)\n",
    "\n",
    "# Merge to associate each RB geometry with its majority control area\n",
    "rb_with_ba = base_gdf[['rb', 'geometry']].merge(majority_ba_by_rb, on='rb')\n",
    "# Filter RBs from p1 to p134\n",
    "rb_with_ba_filtered = rb_with_ba[rb_with_ba['rb'].isin([f'p{i}' for i in range(1, 135)])]\n",
    "\n",
    "# Convert to Web Mercator\n",
    "rb_with_ba_filtered = rb_with_ba_filtered.to_crs(epsg=3857)\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate a wide-range colormap\n",
    "def generate_colormap(n_colors):\n",
    "    colors = []\n",
    "    for i in np.linspace(0, 1, n_colors):\n",
    "        h = i  # Hue\n",
    "        l = 0.5  # Lightness\n",
    "        s = 0.8  # Saturation\n",
    "        rgb = hls_to_rgb(h, l, s)\n",
    "        colors.append(rgb)\n",
    "    return LinearSegmentedColormap.from_list(\"custom_cmap\", colors, N=n_colors)\n",
    "\n",
    "# Assuming rb_filtered_crs is your GeoDataFrame ready for plotting\n",
    "num_unique_areas = rb_with_ba_filtered['BA_Code'].nunique()\n",
    "custom_cmap = generate_colormap(num_unique_areas)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Plot using the generated custom colormap\n",
    "rb_with_ba_filtered.plot(ax=ax, column='BA_Code', legend=True, cmap=custom_cmap,\n",
    "                     edgecolor='black', linewidth=1,\n",
    "                     legend_kwds={'loc': 'upper center', 'bbox_to_anchor': (0.5, -0.15), 'ncol': 3, 'title':'BA_Code'})\n",
    "\n",
    "ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, crs=rb_filtered_crs.crs.to_string())\n",
    "ax.set_title('RBs within BA')\n",
    "\n",
    "for idx, row in rb_filtered_crs.iterrows():\n",
    "    centroid = row.geometry.centroid.coords[0]\n",
    "    plt.text(centroid[0], centroid[1], row['rb'], fontsize=8, ha='center', va='center', color='white')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
