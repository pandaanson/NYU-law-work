{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.4' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOWNLOADING DATA FROM MERRA\n",
      "Predicted time: 48 minutes\n",
      "=====================\n",
      "Downloading temperature data for maputo\n",
      "Time taken for downloading temperature data for maputo: 80.31852006912231 seconds\n",
      "Downloading temperature data for cdelgado\n",
      "Time taken for downloading temperature data for cdelgado: 51.333067178726196 seconds\n",
      "Downloading temperature data for manica\n",
      "Time taken for downloading temperature data for manica: 50.88902711868286 seconds\n",
      "Downloading temperature data for gaza\n",
      "Time taken for downloading temperature data for gaza: 51.11056685447693 seconds\n",
      "CLEANING AND MERGING DATA\n",
      "Predicted time: 0.8 minutes\n",
      "=====================\n",
      "Cleaning and merging temperature data for maputo\n",
      "Cleaning and merging temperature data for cdelgado\n",
      "Cleaning and merging temperature data for manica\n",
      "Cleaning and merging temperature data for gaza\n",
      "FINISHED\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from calendar import monthrange\n",
    "from opendap_download.multi_processing_download import DownloadManager\n",
    "import time\n",
    "####### INPUTS - CHANGE THESE #########\n",
    "folder_location= '/Users/ansonkong/Downloads/merradownload-master/data_test/'\n",
    "username =  # Username for MERRA download account\n",
    "password =  # Password for MERRA download account\n",
    "years = [2007, 2008] # List of years for which data will be downloaded\n",
    "field_id = 'T2M' # ID of field in MERRA-2 - find ID here: https://gmao.gsfc.nasa.gov/pubs/docs/Bosilovich785.pdf \n",
    "field_name = 'temperature' # Name of field to be stored with downloaded data (can use any name you like)\n",
    "database_name = 'M2I1NXASM' # Name of database in which field is stored, can be looked up by ID here: https://gmao.gsfc.nasa.gov/pubs/docs/Bosilovich785.pdf \n",
    "database_id = 'inst1_2d_asm_Nx' # ID of database database in which field is stored, also can be looked up by ID here: https://gmao.gsfc.nasa.gov/pubs/docs/Bosilovich785.pdf \n",
    "locs = [('maputo', -25.9629, 32.5732), # List of locations for which data will be downloaded. Each location is a three-tuple, consisting of name (string), latitude, and longitude floats)\n",
    "        ('cdelgado', -12.3335, 39.3206), \n",
    "        ('manica', -18.9438, 32.8649),\n",
    "        ('gaza', -23.0222, 32.7181)]\n",
    "conversion_function = lambda x: x - 273.15 # Unit conversion function to be applied to daily data. Here is the unit conversion for temperature from Kelvin to Celsius. \n",
    "aggregator = 'mean' # Method by which data will be aggregated over days and weeks. Can be \"sum\", \"mean\", \"min\", or \"max\" (for example, mean will download hourly data, mean daily data, and mean weekly data)\n",
    "\n",
    "####### CONSTANTS - DO NOT CHANGE BELOW THIS LINE #######\n",
    "lat_coords = np.arange(0, 361, dtype=int)\n",
    "lon_coords = np.arange(0, 576, dtype=int)\n",
    "database_url = 'https://goldsmr4.gesdisc.eosdis.nasa.gov/opendap/MERRA2/' + database_name + '.5.12.4/'\n",
    "NUMBER_OF_CONNECTIONS = 5\n",
    "\n",
    "####### DOWNLOAD DATA #########\n",
    "# Translate lat/lon into coordinates that MERRA-2 understands\n",
    "def translate_lat_to_geos5_native(latitude):\n",
    "    \"\"\"\n",
    "    The source for this formula is in the MERRA2 \n",
    "    Variable Details - File specifications for GEOS pdf file.\n",
    "    The Grid in the documentation has points from 1 to 361 and 1 to 576.\n",
    "    The MERRA-2 Portal uses 0 to 360 and 0 to 575.\n",
    "    latitude: float Needs +/- instead of N/S\n",
    "    \"\"\"\n",
    "    return ((latitude + 90) / 0.5)\n",
    "\n",
    "def translate_lon_to_geos5_native(longitude):\n",
    "    \"\"\"See function above\"\"\"\n",
    "    return ((longitude + 180) / 0.625)\n",
    "\n",
    "def find_closest_coordinate(calc_coord, coord_array):\n",
    "    \"\"\"\n",
    "    Since the resolution of the grid is 0.5 x 0.625, the 'real world'\n",
    "    coordinates will not be matched 100% correctly. This function matches \n",
    "    the coordinates as close as possible. \n",
    "    \"\"\"\n",
    "    # np.argmin() finds the smallest value in an array and returns its\n",
    "    # index. np.abs() returns the absolute value of each item of an array.\n",
    "    # To summarize, the function finds the difference closest to 0 and returns \n",
    "    # its index. \n",
    "    index = np.abs(coord_array-calc_coord).argmin()\n",
    "    return coord_array[index]\n",
    "\n",
    "def translate_year_to_file_number(year):\n",
    "    \"\"\"\n",
    "    The file names consist of a number and a meta data string. \n",
    "    The number changes over the years. 1980 until 1991 it is 100, \n",
    "    1992 until 2000 it is 200, 2001 until 2010 it is  300 \n",
    "    and from 2011 until now it is 400.\n",
    "    \"\"\"\n",
    "    file_number = ''\n",
    "    \n",
    "    if year >= 1980 and year < 1992:\n",
    "        file_number = '100'\n",
    "    elif year >= 1992 and year < 2001:\n",
    "        file_number = '200'\n",
    "    elif year >= 2001 and year < 2011:\n",
    "        file_number = '300'\n",
    "    elif year >= 2011:\n",
    "        file_number = '400'\n",
    "    else:\n",
    "        raise Exception('The specified year is out of range.')\n",
    "    return file_number\n",
    "\n",
    "def generate_url_params(parameter, time_para, lat_para, lon_para):\n",
    "    \"\"\"Creates a string containing all the parameters in query form\"\"\"\n",
    "    parameter = map(lambda x: x + time_para, parameter)\n",
    "    parameter = map(lambda x: x + lat_para, parameter)\n",
    "    parameter = map(lambda x: x + lon_para, parameter)\n",
    "    return ','.join(parameter)\n",
    "    \n",
    "def generate_download_links(download_years, base_url, dataset_name, url_params):\n",
    "    \"\"\"\n",
    "    Generates the links for the download. \n",
    "    download_years: The years you want to download as array. \n",
    "    dataset_name: The name of the data set. For example tavg1_2d_slv_Nx\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for y in download_years: \n",
    "        y_str = str(y)\n",
    "        file_num = translate_year_to_file_number(y)\n",
    "        for m in range(1,13):\n",
    "            m_str = str(m).zfill(2)\n",
    "            _, nr_of_days = monthrange(y, m)\n",
    "            for d in range(1,nr_of_days+1):\n",
    "                d_str = str(d).zfill(2)\n",
    "                # Create the file name string\n",
    "                file_name = 'MERRA2_{num}.{name}.{y}{m}{d}.nc4'.format(\n",
    "                    num=file_num, name=dataset_name, \n",
    "                    y=y_str, m=m_str, d=d_str)\n",
    "                # Create the query\n",
    "                query = '{base}{y}/{m}/{name}.nc4?{params}'.format(\n",
    "                    base=base_url, y=y_str, m=m_str, \n",
    "                    name=file_name, params=url_params)\n",
    "                urls.append(query)\n",
    "    return urls\n",
    "\n",
    "print('DOWNLOADING DATA FROM MERRA')\n",
    "print('Predicted time: ' + str(len(years)*len(locs)*6) + ' minutes')\n",
    "print('=====================')\n",
    "for loc, lat, lon in locs:\n",
    "    print('Downloading ' + field_name + ' data for ' + loc)\n",
    "    # Translate the coordinates that define your area to grid coordinates.\n",
    "    lat_coord = translate_lat_to_geos5_native(lat)\n",
    "    lon_coord = translate_lon_to_geos5_native(lon)\n",
    "    # Find the closest coordinate in the grid.\n",
    "    lat_closest = find_closest_coordinate(lat_coord, lat_coords)\n",
    "    lon_closest = find_closest_coordinate(lon_coord, lon_coords)\n",
    "    # Generate URLs for scraping\n",
    "    requested_lat = '[{lat}:1:{lat}]'.format(lat=lat_closest)\n",
    "    requested_lon = '[{lon}:1:{lon}]'.format(lon=lon_closest)\n",
    "    parameter = generate_url_params([field_id], '[0:1:23]', requested_lat, requested_lon)\n",
    "    generated_URL = generate_download_links(years, database_url, database_id, parameter)\n",
    "    download_manager = DownloadManager(download_path=folder_location)\n",
    "    download_manager.set_username_and_password(username, password)\n",
    "    download_manager.download_path = folder_location+field_name + '/' + loc\n",
    "    download_manager.download_urls = generated_URL\n",
    "    # Start measuring time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Start the download\n",
    "    download_manager.start_download(NUMBER_OF_CONNECTIONS)\n",
    "    \n",
    "    # Stop measuring time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate and print the time taken\n",
    "    time_taken = end_time - start_time\n",
    "    print(f\"Time taken for downloading {field_name} data for {loc}: {time_taken} seconds\")\n",
    "\n",
    "\n",
    "######### OPEN, CLEAN, MERGE, MERGE DATA AND WRITE CSVS ##########\n",
    "def extract_date(data_set):\n",
    "    \"\"\"\n",
    "    Extracts the date from the filename before merging the datasets. \n",
    "    \"\"\" \n",
    "    if 'HDF5_GLOBAL.Filename' in data_set.attrs:\n",
    "        f_name = data_set.attrs['HDF5_GLOBAL.Filename']\n",
    "    elif 'Filename' in data_set.attrs:\n",
    "        f_name = data_set.attrs['Filename']\n",
    "    else: \n",
    "        raise AttributeError('The attribute name has changed again!')\n",
    "    # find a match between \".\" and \".nc4\" that does not have \".\" .\n",
    "    exp = r'(?<=\\.)[^\\.]*(?=\\.nc4)'\n",
    "    res = re.search(exp, f_name).group(0)\n",
    "    # Extract the date. \n",
    "    y, m, d = res[0:4], res[4:6], res[6:8]\n",
    "    date_str = ('%s-%s-%s' % (y, m, d))\n",
    "    data_set = data_set.assign(date=date_str)\n",
    "    return data_set\n",
    "\n",
    "# Open nc4 files as dataframes, perform aggregations and save as CSV files\n",
    "print('CLEANING AND MERGING DATA')\n",
    "print('Predicted time: ' + str(len(years)*len(locs)*0.1) + ' minutes')\n",
    "print('=====================')\n",
    "for loc, lat, lon in locs:\n",
    "    print('Cleaning and merging ' + field_name + ' data for ' + loc)\n",
    "    dfs = []\n",
    "    for file in os.listdir(folder_location+field_name + '/' + loc):\n",
    "        if '.nc4' in file:\n",
    "            file_path=folder_location+field_name + '/' + loc + '/' + file\n",
    "            try:\n",
    "                with xr.open_mfdataset(file_path, preprocess=extract_date) as df:\n",
    "                    dfs.append(df.to_dataframe())\n",
    "            except Exception as e:  # Catch the exception and store it as variable e\n",
    "                print(f\"Issue with file {file_path}: {e}\")\n",
    "    df_hourly = pd.concat(dfs)\n",
    "    df_hourly['time'] = df_hourly.index.get_level_values(level=2)\n",
    "    df_hourly.columns = [field_name, 'date', 'time']\n",
    "    df_hourly[field_name] = df_hourly[field_name].apply(conversion_function)\n",
    "    df_hourly['date'] = pd.to_datetime(df_hourly['date'])\n",
    "    df_hourly.to_csv(folder_location+field_name + '/' + loc + '_hourly.csv', header=[field_name, 'date', 'time'], index=False)\n",
    "    df_hourly = pd.read_csv(folder_location+field_name + '/' + loc + '_hourly.csv')\n",
    "    #The orginal file include aggreation which is not needed in our case\n",
    "    # df_daily = df_hourly.groupby('date').agg(aggregator)\n",
    "    # df_daily = df_daily.drop('time', axis=1)\n",
    "    # df_daily['date'] = df_daily.index\n",
    "    # df_daily.to_csv(field_name + '/' + loc + '_daily.csv', header=[field_name, 'date'], index=False)\n",
    "    # df_weekly = df_daily\n",
    "    # df_weekly['Week'] = pd.to_datetime(df_weekly['date']).apply(lambda x: x.isocalendar()[1])\n",
    "    # df_weekly['Year'] = pd.to_datetime(df_weekly['date']).apply(lambda x: x.year)\n",
    "    # df_weekly = df_weekly.groupby(['Year', 'Week']).agg(aggregator)\n",
    "    # df_weekly['Year'] = df_weekly.index.get_level_values(0)\n",
    "    # df_weekly['Week'] = df_weekly.index.get_level_values(1)\n",
    "    # df_weekly.to_csv(field_name + '/' + loc + '_weekly.csv', index=False)\n",
    "\n",
    "print('FINISHED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tell\n",
    "# Start by importing the TELL package and information about your operating system:\n",
    "import os\n",
    "import tell\n",
    "# Identify the current working directory, the subdirectory where the data will be stored, and the image output subdirectory:\n",
    "current_dir =  os.path.join(os.path.dirname(os.getcwd()))\n",
    "tell_data_dir = os.path.join(current_dir, r'tell_data')\n",
    "tell_image_dir = os.path.join(tell_data_dir, r'visualizations')\n",
    "\n",
    "# If the \"tell_data_dir\" subdirectory doesn't exist then create it:\n",
    "if not os.path.exists(tell_data_dir):\n",
    "   os.makedirs(tell_data_dir)\n",
    "\n",
    "# If the \"tell_image_dir\" subdirectory doesn't exist then create it:\n",
    "if not os.path.exists(tell_image_dir):\n",
    "   os.makedirs(tell_image_dir)\n",
    "\n",
    "# Download the TELL quickstarter data package from Zenodo:\n",
    "tell.install_quickstarter_data(data_dir = tell_data_dir)\n",
    "# Run the MLP training step for a single BA (i.e., \"region\"):\n",
    "prediction_df, validation_df = tell.train(region = 'PJM',\n",
    "                                          data_dir = os.path.join(tell_data_dir, r'tell_quickstarter_data', r'outputs', r'compiled_historical_data'))\n",
    "\n",
    "# View the head of the prediction dataframe that contains the time-series of projected load in the evaluation year:\n",
    "display(prediction_df.head(10))\n",
    "\n",
    "# View validation dataframe that contains error statistics for the trained model:\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  (time: 24, lat: 1, lon: 1)\n",
      "Dimensions without coordinates: time, lat, lon\n",
      "Data variables:\n",
      "    T2M      (time, lat, lon) float32 ...\n",
      "Attributes: (12/32)\n",
      "    History:                           Original file generated: Sun Feb 15 07...\n",
      "    Comment:                           GMAO filename: d5124_m2_jan00.inst1_2d...\n",
      "    Filename:                          MERRA2_300.inst1_2d_asm_Nx.20081228.nc4\n",
      "    Conventions:                       CF-1\n",
      "    Institution:                       NASA Global Modeling and Assimilation ...\n",
      "    References:                        http://gmao.gsfc.nasa.gov\n",
      "    ...                                ...\n",
      "    RangeBeginningDate:                2008-12-28\n",
      "    RangeBeginningTime:                00:00:00.000000\n",
      "    RangeEndingDate:                   2008-12-28\n",
      "    RangeEndingTime:                   23:00:00.000000\n",
      "    DODS_EXTRA.Unlimited_Dimension:    time\n",
      "    history:                           2024-02-13 23:49:27 GMT Hyrax-1.16.3 h...\n"
     ]
    }
   ],
   "source": [
    "# import xarray as xr\n",
    "\n",
    "# # Replace this with the path to your actual file\n",
    "# file_path = \"/Users/ansonkong/Downloads/merradownload-master/data_test/temperature/maputo/MERRA2_300.inst1_2d_asm_Nx.20081228.nc4\"\n",
    "\n",
    "# ds = xr.open_dataset(file_path)\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temperature</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.734918</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.323602</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.935632</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.684503</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.270166</td>\n",
       "      <td>2008-12-28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17539</th>\n",
       "      <td>24.760095</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17540</th>\n",
       "      <td>24.198419</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17541</th>\n",
       "      <td>23.777673</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17542</th>\n",
       "      <td>23.486108</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17543</th>\n",
       "      <td>23.224878</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       temperature        date  time\n",
       "0        21.734918  2008-12-28     0\n",
       "1        21.323602  2008-12-28     0\n",
       "2        20.935632  2008-12-28     0\n",
       "3        20.684503  2008-12-28     0\n",
       "4        21.270166  2008-12-28     0\n",
       "...            ...         ...   ...\n",
       "17539    24.760095  2008-12-31     0\n",
       "17540    24.198419  2008-12-31     0\n",
       "17541    23.777673  2008-12-31     0\n",
       "17542    23.486108  2008-12-31     0\n",
       "17543    23.224878  2008-12-31     0\n",
       "\n",
       "[17544 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hourly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
