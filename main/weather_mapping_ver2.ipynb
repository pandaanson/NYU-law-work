{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of code, the goal is to go from mapping and county level weather data to to subgroup data\n",
    "# Define paths\n",
    "data_and_output_dir = '/Users/ansonkong/Downloads/Data for nyu work/'\n",
    "full_future_data_path = '/Volumes/T7/weather2/rcp85hotter/'\n",
    "full_historical_data_path = '/Volumes/T7/weather2/historic/'\n",
    "\n",
    "#this just smooth out operation\n",
    "cutoff_year=2020 #The year prediction dataset start\n",
    "start_year=2015 #The year historical \n",
    "end_year=2100\n",
    "\n",
    "#Debug mode(only output once)\n",
    "debug=False\n",
    "#create population level data\n",
    "#import require file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>county</th>\n",
       "      <th>state</th>\n",
       "      <th>cnty_fips</th>\n",
       "      <th>reeds_region</th>\n",
       "      <th>reeds_ba</th>\n",
       "      <th>reeds_raz</th>\n",
       "      <th>lbnl_region</th>\n",
       "      <th>nrel_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chelan</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53007</td>\n",
       "      <td>s4</td>\n",
       "      <td>p1</td>\n",
       "      <td>WECC Pacific Northwest</td>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kittitas</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53037</td>\n",
       "      <td>s4</td>\n",
       "      <td>p1</td>\n",
       "      <td>WECC Pacific Northwest</td>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Island</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53029</td>\n",
       "      <td>s3</td>\n",
       "      <td>p1</td>\n",
       "      <td>WECC Pacific Northwest</td>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>King</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53033</td>\n",
       "      <td>s3</td>\n",
       "      <td>p1</td>\n",
       "      <td>WECC Pacific Northwest</td>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kitsap</td>\n",
       "      <td>Washington</td>\n",
       "      <td>53035</td>\n",
       "      <td>s3</td>\n",
       "      <td>p1</td>\n",
       "      <td>WECC Pacific Northwest</td>\n",
       "      <td>West</td>\n",
       "      <td>Pacific</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3113</th>\n",
       "      <td>Aroostook</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23003</td>\n",
       "      <td>s355</td>\n",
       "      <td>p134</td>\n",
       "      <td>NPCC New England</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>Franklin</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23007</td>\n",
       "      <td>s355</td>\n",
       "      <td>p134</td>\n",
       "      <td>NPCC New England</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3115</th>\n",
       "      <td>Oxford</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23017</td>\n",
       "      <td>s355</td>\n",
       "      <td>p134</td>\n",
       "      <td>NPCC New England</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3116</th>\n",
       "      <td>Piscataquis</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23021</td>\n",
       "      <td>s355</td>\n",
       "      <td>p134</td>\n",
       "      <td>NPCC New England</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3117</th>\n",
       "      <td>Somerset</td>\n",
       "      <td>Maine</td>\n",
       "      <td>23025</td>\n",
       "      <td>s355</td>\n",
       "      <td>p134</td>\n",
       "      <td>NPCC New England</td>\n",
       "      <td>Northeast</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3118 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           county       state  cnty_fips reeds_region reeds_ba  \\\n",
       "0          Chelan  Washington      53007           s4       p1   \n",
       "1        Kittitas  Washington      53037           s4       p1   \n",
       "2          Island  Washington      53029           s3       p1   \n",
       "3            King  Washington      53033           s3       p1   \n",
       "4          Kitsap  Washington      53035           s3       p1   \n",
       "...           ...         ...        ...          ...      ...   \n",
       "3113    Aroostook       Maine      23003         s355     p134   \n",
       "3114     Franklin       Maine      23007         s355     p134   \n",
       "3115       Oxford       Maine      23017         s355     p134   \n",
       "3116  Piscataquis       Maine      23021         s355     p134   \n",
       "3117     Somerset       Maine      23025         s355     p134   \n",
       "\n",
       "                   reeds_raz lbnl_region nrel_region  \n",
       "0     WECC Pacific Northwest        West     Pacific  \n",
       "1     WECC Pacific Northwest        West     Pacific  \n",
       "2     WECC Pacific Northwest        West     Pacific  \n",
       "3     WECC Pacific Northwest        West     Pacific  \n",
       "4     WECC Pacific Northwest        West     Pacific  \n",
       "...                      ...         ...         ...  \n",
       "3113        NPCC New England   Northeast   Northeast  \n",
       "3114        NPCC New England   Northeast   Northeast  \n",
       "3115        NPCC New England   Northeast   Northeast  \n",
       "3116        NPCC New England   Northeast   Northeast  \n",
       "3117        NPCC New England   Northeast   Northeast  \n",
       "\n",
       "[3118 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping\n",
    "mapping_path = data_and_output_dir + 'Input/county_map.csv'\n",
    "mapping_df = pd.read_csv(mapping_path)\n",
    "mapping_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'09140', '09170', '09120', '09150', '09110', '09130', '09160', '09180', '09190'}\n",
      "county\n",
      "state\n",
      "cnty_fips\n",
      "reeds_region\n",
      "reeds_ba\n",
      "reeds_raz\n",
      "lbnl_region\n",
      "nrel_region\n",
      "FIPS\n",
      "2015\n",
      "2016\n",
      "2017\n",
      "2018\n",
      "2019\n",
      "2020\n",
      "2021\n",
      "2022\n",
      "2023\n",
      "2024\n",
      "2025\n",
      "2026\n",
      "2027\n",
      "2028\n",
      "2029\n",
      "2030\n",
      "2031\n",
      "2032\n",
      "2033\n",
      "2034\n",
      "2035\n",
      "2036\n",
      "2037\n",
      "2038\n",
      "2039\n",
      "2040\n",
      "2041\n",
      "2042\n",
      "2043\n",
      "2044\n",
      "2045\n",
      "2046\n",
      "2047\n",
      "2048\n",
      "2049\n",
      "2050\n",
      "2051\n",
      "2052\n",
      "2053\n",
      "2054\n",
      "2055\n",
      "2056\n",
      "2057\n",
      "2058\n",
      "2059\n",
      "2060\n",
      "2061\n",
      "2062\n",
      "2063\n",
      "2064\n",
      "2065\n",
      "2066\n",
      "2067\n",
      "2068\n",
      "2069\n",
      "2070\n",
      "2071\n",
      "2072\n",
      "2073\n",
      "2074\n",
      "2075\n",
      "2076\n",
      "2077\n",
      "2078\n",
      "2079\n",
      "2080\n",
      "2081\n",
      "2082\n",
      "2083\n",
      "2084\n",
      "2085\n",
      "2086\n",
      "2087\n",
      "2088\n",
      "2089\n",
      "2090\n",
      "2091\n",
      "2092\n",
      "2093\n",
      "2094\n",
      "2095\n",
      "2096\n",
      "2097\n",
      "2098\n",
      "2099\n",
      "2100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Path, this will update itself\n",
    "mapping_path = data_and_output_dir + 'Input/county_map.csv'\n",
    "future_population_data_path = data_and_output_dir + 'input/Electric_Retail_Service_Territories/ssp3_county_population.csv'\n",
    "historical_population_data_path = data_and_output_dir + 'input/county_populations_2000_to_2020.csv'\n",
    "output_path = data_and_output_dir + 'output/'\n",
    "\n",
    "# Load mapping and population data\n",
    "mapping_df = pd.read_csv(mapping_path)\n",
    "future_population_df = pd.read_csv(future_population_data_path)\n",
    "historical_population_df = pd.read_csv(historical_population_data_path)\n",
    "\n",
    "#crate looping for interp\n",
    "years = np.arange(cutoff_year, end_year + 1, 10)\n",
    "interp_years = np.arange(cutoff_year, end_year + 1)\n",
    "\n",
    "# Ensure FIPS codes are five zero-padded strings\n",
    "historical_population_df['FIPS'] = historical_population_df['county_FIPS'].astype(str).str.zfill(5)\n",
    "future_population_df['FIPS'] = future_population_df['FIPS'].astype(str).str.zfill(5)\n",
    "mapping_df['cnty_fips'] = mapping_df['cnty_fips'].astype(str).str.zfill(5)\n",
    "\n",
    "# Rename 'pop_{year}' columns to just '{year}'\n",
    "for year in range(start_year, cutoff_year+1):\n",
    "    if f'pop_{year}' in historical_population_df.columns:\n",
    "        historical_population_df.rename(columns={f'pop_{year}': str(year)}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'FIPS' is the column to join on and 'Year' is the column indicating the year in future_population_df\n",
    "for index, row in future_population_df.iterrows():\n",
    "    pop_values = [row[str(year)] for year in years if str(year) in row]\n",
    "    interpolator = interp1d(years, pop_values, kind='linear', fill_value=\"extrapolate\")\n",
    "    interpolated_values = interpolator(interp_years)\n",
    "    for year in interp_years:\n",
    "        future_population_df.at[index, str(year)] = interpolated_values[year - cutoff_year]\n",
    "\n",
    "\n",
    "# Select relevant columns for historical data\n",
    "historical_population_df = historical_population_df[['FIPS'] + [str(year) for year in range(start_year, cutoff_year)]]\n",
    "\n",
    "# Select relevant columns for future data\n",
    "future_population_df = future_population_df[['FIPS'] + [str(year) for year in interp_years]]\n",
    "\n",
    "# Concatenate historical and future dataframes\n",
    "combined_population_df = pd.merge(historical_population_df, future_population_df, on='FIPS', how='outer')\n",
    "# Merge combined population data with mapping data\n",
    "# Assuming 'GEOID' in mapping_df corresponds to 'FIPS' in population data\n",
    "combined_df = pd.merge(mapping_df, combined_population_df, left_on='cnty_fips', right_on='FIPS', how='inner')\n",
    "combined_df['FIPS']=combined_df['cnty_fips']\n",
    "\n",
    "# Identify FIPS codes in mapping_df that did not successfully join\n",
    "missing_in_combined = set(mapping_df['cnty_fips']) - set(combined_df['cnty_fips'])\n",
    "print(missing_in_combined)\n",
    "for column in combined_df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All expected 'reeds_ba' values from 'p1' to 'p134' are present in combined_df.\n"
     ]
    }
   ],
   "source": [
    "# Extract unique 'reeds_ba' values from combined_df\n",
    "unique_reeds_ba = set(combined_df['reeds_ba'].unique())\n",
    "\n",
    "# Generate a set of expected 'reeds_ba' values ('p1' to 'p134')\n",
    "expected_reeds_ba = set([f'p{i}' for i in range(1, 135)])\n",
    "\n",
    "# Find missing and extra values\n",
    "missing_reeds_ba = expected_reeds_ba - unique_reeds_ba\n",
    "extra_reeds_ba = unique_reeds_ba - expected_reeds_ba\n",
    "\n",
    "# Print results\n",
    "if not missing_reeds_ba:\n",
    "    print(\"All expected 'reeds_ba' values from 'p1' to 'p134' are present in combined_df.\")\n",
    "else:\n",
    "    print(f\"Missing 'reeds_ba' values in combined_df: {missing_reeds_ba}\")\n",
    "\n",
    "if extra_reeds_ba:\n",
    "    print(f\"Extra 'reeds_ba' values in combined_df not in the range 'p1' to 'p134': {extra_reeds_ba}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process start for year: 2015\n"
     ]
    }
   ],
   "source": [
    "# main\n",
    "\n",
    "historical_weather_path = os.path.join(output_path, 'historical_weather')\n",
    "future_weather_path = os.path.join(output_path, 'future_weather')\n",
    "os.makedirs(historical_weather_path, exist_ok=True)\n",
    "os.makedirs(future_weather_path, exist_ok=True)\n",
    "# Function to load checkpoint\n",
    "def load_checkpoint(year):\n",
    "    checkpoint_file = os.path.join(output_path, 'checkpoint.txt')\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as file:\n",
    "            completed_years = file.read().splitlines()\n",
    "        return str(year) in completed_years\n",
    "    return False\n",
    "\n",
    "# Function to save checkpoint\n",
    "def save_checkpoint(year):\n",
    "    checkpoint_file = os.path.join(output_path, 'checkpoint.txt')\n",
    "    with open(checkpoint_file, 'a') as file:\n",
    "        file.write(f\"{year}\\n\")\n",
    "yearloopbreaker=False\n",
    "fileloopbreaker=False\n",
    "rbloopbreaker=False\n",
    "for year in range(start_year, end_year + 1):\n",
    "\n",
    "    if yearloopbreaker:break\n",
    "    if load_checkpoint(year):\n",
    "        print(f\"Year {year} already processed. Skipping.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    print(f'Process start for year: {year}')\n",
    "    if year < cutoff_year:\n",
    "        folder_to_read = os.path.join(full_historical_data_path, str(year))\n",
    "        folder_to_store = historical_weather_path\n",
    "    else:\n",
    "        folder_to_read = os.path.join(full_future_data_path, str(year))\n",
    "        folder_to_store = future_weather_path\n",
    "    \n",
    "    # Initialize a dictionary to hold DataFrames for each rb\n",
    "    rb_dfs = {}\n",
    "    # Sort files before processing\n",
    "    files = sorted(glob(os.path.join(folder_to_read, '*.csv')))\n",
    "    total_files = len(files)\n",
    "    processed_files = 0\n",
    "\n",
    "    # Iterate through each file in the year's folder\n",
    "    for file_path in files:\n",
    "        processed_files += 1\n",
    "        #print(f\"Processing file {processed_files}/{total_files} ({(processed_files/total_files)*100:.2f}%)\")\n",
    "\n",
    "        if fileloopbreaker:break\n",
    "        \n",
    "        meteorology_df = pd.read_csv(file_path)\n",
    "        if meteorology_df.empty:\n",
    "            print(f\"Skipped empty file: {file_path}\")\n",
    "            continue\n",
    "        #print(meteorology_df.columns)\n",
    "        meteorology_df['FIPS'] = meteorology_df['FIPS'].astype(str).str.zfill(5)\n",
    "        meteorology_df['WSPD'] = np.sqrt(meteorology_df['U10']**2 + meteorology_df['V10']**2)\n",
    "        \n",
    "        # Extract date and time from file name\n",
    "        _, month, day, hour = map(int, os.path.basename(file_path).split('_')[:4])\n",
    "        utc_time = datetime(year, month, day, hour).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        meteorology_df['Time_UTC'] = utc_time\n",
    "        \n",
    "        # Merge with background data\n",
    "        merged_df = pd.merge(combined_df, meteorology_df, on='FIPS', how='inner')\n",
    "        \n",
    "        # Calculate weighted averages and accumulate in rb_dfs\n",
    "        merged_df['Population'] = merged_df[str(year)]\n",
    "        for rb, group in merged_df.groupby('reeds_ba'):\n",
    "            if rbloopbreaker:break\n",
    "            \n",
    "            if rb not in rb_dfs:\n",
    "                rb_dfs[rb] = pd.DataFrame()\n",
    "            weighted = group.copy()\n",
    "            \n",
    "            for col in ['T2', 'Q2', 'SWDOWN', 'GLW', 'WSPD']:\n",
    "                weighted[col] = weighted[col] * weighted['Population']\n",
    "            weighted_sum = weighted.groupby('reeds_ba').sum()\n",
    "            for col in ['T2', 'Q2', 'SWDOWN', 'GLW', 'WSPD']:\n",
    "                weighted_sum[col] = weighted_sum[col] / weighted_sum['Population']\n",
    "            weighted_sum['Time_UTC'] = utc_time\n",
    "            weighted_sum=weighted_sum[['Time_UTC','T2', 'Q2', 'SWDOWN', 'GLW', 'WSPD']]\n",
    "            rb_dfs[rb] = pd.concat([rb_dfs[rb], weighted_sum.reset_index()], ignore_index=True)\n",
    "        if debug:\n",
    "            fileloopbreaker=True\n",
    "            yearloopbreaker=True\n",
    "    save_checkpoint(year)\n",
    "\n",
    "    \n",
    "    # Save each RB's accumulated data to its file\n",
    "    for rb, df in rb_dfs.items():\n",
    "        output_file = os.path.join(folder_to_store, f\"{rb}_WRF_Hourly_Mean_Meteorology_{year}.csv\")\n",
    "        df.sort_values(by='Time_UTC', inplace=True)\n",
    "        df[['Time_UTC','T2', 'Q2', 'SWDOWN', 'GLW', 'WSPD']].to_csv(output_file, index=False)\n",
    "        print(f\"Processed and saved: {output_file} for RB: {rb}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
