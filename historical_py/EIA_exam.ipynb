{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_15603/598961565.py:4: DtypeWarning: Columns (5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  balance_df = pd.read_csv('/Users/ansonkong/Downloads/Data for nyu work/Input/EIA 930/BA/EIA930_BALANCE_2020_Jan_Jun.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct Sub-Region: 82\n",
      "Number of distinct Balancing Authority in SUBREGION: 8\n",
      "Number of distinct Balancing Authority in BALANCE not in SUBREGION: 45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "balance_df = pd.read_csv('/Users/ansonkong/Downloads/Data for nyu work/Input/EIA 930/BA/EIA930_BALANCE_2020_Jan_Jun.csv')\n",
    "subregion_df = pd.read_csv('/Users/ansonkong/Downloads/Data for nyu work/Input/EIA 930/Subregion/EIA930_SUBREGION_2020_Jan_Jun.csv')\n",
    "\n",
    "# Filter the datasets\n",
    "balance_df_filtered = balance_df[(balance_df['UTC Time at End of Hour'] == '02/01/2020 7:00:00 AM')& (balance_df['Demand (MW)'].notna())]\n",
    "subregion_df_filtered = subregion_df[(subregion_df['UTC Time at End of Hour'] == '02/01/2020 7:00:00 AM') & (subregion_df['Demand (MW)'].notna())]\n",
    "\n",
    "# Get the number of distinct Sub-Region and Balancing Authority for SUBREGION\n",
    "distinct_sub_region = subregion_df_filtered['Sub-Region'].nunique()\n",
    "distinct_ba_subregion = subregion_df_filtered['Balancing Authority'].nunique()\n",
    "\n",
    "# For BALANCE, find distinct Balancing Authority not in SUBREGION\n",
    "distinct_ba_balance = balance_df_filtered[~balance_df_filtered['Balancing Authority'].isin(subregion_df_filtered['Balancing Authority'])]['Balancing Authority'].nunique()\n",
    "\n",
    "print(f\"Number of distinct Sub-Region: {distinct_sub_region}\")\n",
    "print(f\"Number of distinct Balancing Authority in SUBREGION: {distinct_ba_subregion}\")\n",
    "print(f\"Number of distinct Balancing Authority in BALANCE not in SUBREGION: {distinct_ba_balance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre define path\n",
    "data_and_output_dir = '/Users/ansonkong/Downloads/Data for nyu work/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths to the directories\n",
    "subregion_dir = data_and_output_dir+'Input/EIA 930/Subregion/'\n",
    "ba_dir = data_and_output_dir+'Input/EIA 930/BA/'\n",
    "\n",
    "# Function to read and merge CSVs from a directory\n",
    "def read_and_merge_csvs(directory, cols):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# Columns to keep for each dataset\n",
    "subregion_cols = ['UTC Time at End of Hour', 'Demand (MW)', 'Sub-Region', 'Balancing Authority']\n",
    "ba_cols = ['UTC Time at End of Hour', 'Balancing Authority', 'Demand (MW)']\n",
    "\n",
    "# Read and merge CSVs\n",
    "subregion_combined_df = read_and_merge_csvs(subregion_dir, subregion_cols)\n",
    "ba_combined_df = read_and_merge_csvs(ba_dir, ba_cols)\n",
    "\n",
    "# Before the loop, initialize an empty list to collect new rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate over BA combined dataframe to check for missing Balancing Authorities in Subregion dataframe\n",
    "for index, row in ba_combined_df.iterrows():\n",
    "    matching_rows = subregion_combined_df[(subregion_combined_df['UTC Time at End of Hour'] == row['UTC Time at End of Hour']) & \n",
    "                                           (subregion_combined_df['Balancing Authority'] == row['Balancing Authority'])]\n",
    "    if matching_rows.empty:\n",
    "        # If no matching row, prepare the new row with adjustments\n",
    "        new_row = row.to_dict()\n",
    "        new_row['Sub-Region'] = new_row['Balancing Authority']  # Set Sub-Region to Balancing Authority value\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# After the loop, add all new rows to the dataframe at once\n",
    "if new_rows:  # Check if there are any new rows to add\n",
    "    subregion_combined_df = pd.concat([subregion_combined_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# You may now use subregion_combined_df with the added records.\n",
    "# Optionally, you can save this dataframe to a new CSV file\n",
    "subregion_combined_df.to_csv(data_and_output_dir+'output/merged_subregion_with_ba.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the shapefile:\n",
      "Index(['STATEFP', 'COUNTYFP', 'COUNTYNS', 'AFFGEOID', 'GEOID', 'NAME', 'LSAD',\n",
      "       'ALAND', 'AWATER', 'geometry'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns in the CSV file:\n",
      "Index(['Year', 'State_FIPS', 'State_Name', 'County_FIPS', 'County_Name',\n",
      "       'BA_Number', 'BA_Code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming data_and_output_dir is defined and contains the path to your data directory\n",
    "data_and_output_dir = '/Users/ansonkong/Downloads/Data for nyu work/' # Update this path\n",
    "\n",
    "# Path to the shapefile and CSV\n",
    "shapefile_path = data_and_output_dir + 'Input/cb_2018_us_county_500k/cb_2018_us_county_500k.shp'\n",
    "csv_path = data_and_output_dir + 'Input/ba_service_territory_2020.csv'\n",
    "\n",
    "# Read the shapefile using GeoPandas\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Read the CSV file. Use GeoPandas if you expect to work with geographic data in the CSV.\n",
    "# If the CSV doesn't contain geographic data, you could just use pandas with pd.read_csv(csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Print all columns from the shapefile\n",
    "print(\"Columns in the shapefile:\")\n",
    "print(gdf.columns)\n",
    "\n",
    "# Print all columns from the CSV file\n",
    "print(\"\\nColumns in the CSV file:\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('/Users/ansonkong/Downloads/2007.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (8760, 134)\n",
      "Number of hours: 8760\n",
      "Time dimension is correct for a non-leap year.\n",
      "Number of p_zones: 134\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data = np.load('/Users/ansonkong/Downloads/2007.npy')\n",
    "\n",
    "# Check the shape of the loaded array\n",
    "print(\"Data shape:\", data.shape)\n",
    "\n",
    "# Assuming the first axis represents time and the second represents p_zones\n",
    "num_hours, num_p_zones = data.shape\n",
    "\n",
    "# Validate the time dimension\n",
    "print(\"Number of hours:\", num_hours)\n",
    "if num_hours == 8760:\n",
    "    print(\"Time dimension is correct for a non-leap year.\")\n",
    "else:\n",
    "    print(\"Time dimension mismatch. Expected 8760 for a non-leap year.\")\n",
    "\n",
    "# Check the ordering and count of p_zones\n",
    "print(\"Number of p_zones:\", num_p_zones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         p1      p10   p100     p101    p102    p103   p104  \\\n",
      "2007-01-01 00:00:00  5317.0  12190.0  256.0  11816.0  6722.0  8051.0  766.0   \n",
      "2007-01-01 01:00:00  5122.0  11580.0  243.0  11178.0  6359.0  7907.0  746.0   \n",
      "2007-01-01 02:00:00  5025.0  11101.0  237.0  10339.0  5881.0  7768.0  733.0   \n",
      "2007-01-01 03:00:00  4992.0  10804.0  237.0   9877.0  5618.0  7524.0  724.0   \n",
      "2007-01-01 04:00:00  5088.0  10690.0  241.0   9614.0  5469.0  7442.0  731.0   \n",
      "\n",
      "                       p105    p106    p107  ...     p90    p91     p92  \\\n",
      "2007-01-01 00:00:00  5410.0  1464.0  2317.0  ...  3340.0  952.0  8834.0   \n",
      "2007-01-01 01:00:00  5264.0  1427.0  2254.0  ...  3256.0  926.0  8773.0   \n",
      "2007-01-01 02:00:00  5131.0  1390.0  2207.0  ...  3201.0  909.0  8708.0   \n",
      "2007-01-01 03:00:00  5098.0  1376.0  2172.0  ...  3185.0  904.0  8714.0   \n",
      "2007-01-01 04:00:00  5123.0  1376.0  2165.0  ...  3218.0  912.0  8865.0   \n",
      "\n",
      "                        p93      p94     p95     p96     p97     p98     p99  \n",
      "2007-01-01 00:00:00  1343.0  11412.0  2105.0  4519.0  5424.0  5168.0  7692.0  \n",
      "2007-01-01 01:00:00  1326.0  10906.0  2019.0  4323.0  5210.0  4996.0  7411.0  \n",
      "2007-01-01 02:00:00  1319.0  10650.0  1931.0  4140.0  5000.0  4784.0  7221.0  \n",
      "2007-01-01 03:00:00  1323.0  10421.0  1887.0  4007.0  4885.0  4664.0  7153.0  \n",
      "2007-01-01 04:00:00  1348.0  10425.0  1890.0  4002.0  4880.0  4644.0  7336.0  \n",
      "\n",
      "[5 rows x 134 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_15853/3972925922.py:8: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  time_index = pd.date_range(start='2007-01-01', periods=data.shape[0], freq='H')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Create p_zone labels\n",
    "p_zone_labels = [f'p{i}' for i in range(1, 135)]\n",
    "# Correctly ordering p_zone_labels numerically\n",
    "p_zone_labels_sorted = sorted(p_zone_labels, key=lambda x: x)\n",
    "\n",
    "# Creating a time index for the DataFrame\n",
    "time_index = pd.date_range(start='2007-01-01', periods=data.shape[0], freq='H')\n",
    "\n",
    "# Constructing the DataFrame with correctly ordered p_zone_labels\n",
    "df = pd.DataFrame(data, index=time_index, columns=p_zone_labels_sorted)\n",
    "\n",
    "# Displaying the first few rows of the DataFrame to check its structure\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
