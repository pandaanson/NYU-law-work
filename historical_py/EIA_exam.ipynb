{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_15603/598961565.py:4: DtypeWarning: Columns (5,6,7,8,9,10,11,13,14,15,16,17,18,19,20,21,22,23,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  balance_df = pd.read_csv('/Users/ansonkong/Downloads/Data for nyu work/Input/EIA 930/BA/EIA930_BALANCE_2020_Jan_Jun.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct Sub-Region: 82\n",
      "Number of distinct Balancing Authority in SUBREGION: 8\n",
      "Number of distinct Balancing Authority in BALANCE not in SUBREGION: 45\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "balance_df = pd.read_csv('/Users/ansonkong/Downloads/Data for nyu work/Input/EIA 930/BA/EIA930_BALANCE_2020_Jan_Jun.csv')\n",
    "subregion_df = pd.read_csv('/Users/ansonkong/Downloads/Data for nyu work/Input/EIA 930/Subregion/EIA930_SUBREGION_2020_Jan_Jun.csv')\n",
    "\n",
    "# Filter the datasets\n",
    "balance_df_filtered = balance_df[(balance_df['UTC Time at End of Hour'] == '02/01/2020 7:00:00 AM')& (balance_df['Demand (MW)'].notna())]\n",
    "subregion_df_filtered = subregion_df[(subregion_df['UTC Time at End of Hour'] == '02/01/2020 7:00:00 AM') & (subregion_df['Demand (MW)'].notna())]\n",
    "\n",
    "# Get the number of distinct Sub-Region and Balancing Authority for SUBREGION\n",
    "distinct_sub_region = subregion_df_filtered['Sub-Region'].nunique()\n",
    "distinct_ba_subregion = subregion_df_filtered['Balancing Authority'].nunique()\n",
    "\n",
    "# For BALANCE, find distinct Balancing Authority not in SUBREGION\n",
    "distinct_ba_balance = balance_df_filtered[~balance_df_filtered['Balancing Authority'].isin(subregion_df_filtered['Balancing Authority'])]['Balancing Authority'].nunique()\n",
    "\n",
    "print(f\"Number of distinct Sub-Region: {distinct_sub_region}\")\n",
    "print(f\"Number of distinct Balancing Authority in SUBREGION: {distinct_ba_subregion}\")\n",
    "print(f\"Number of distinct Balancing Authority in BALANCE not in SUBREGION: {distinct_ba_balance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre define path\n",
    "data_and_output_dir = '/Users/ansonkong/Downloads/Data for nyu work/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_1417/3776213816.py:13: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Paths to the directories\n",
    "subregion_dir = data_and_output_dir+'Input/EIA 930/Subregion/'\n",
    "ba_dir = data_and_output_dir+'Input/EIA 930/BA/'\n",
    "\n",
    "# Function to read and merge CSVs from a directory\n",
    "def read_and_merge_csvs(directory, cols):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# Columns to keep for each dataset\n",
    "subregion_cols = ['UTC Time at End of Hour', 'Demand (MW)', 'Sub-Region', 'Balancing Authority']\n",
    "ba_cols = ['UTC Time at End of Hour', 'Balancing Authority', 'Demand (MW)']\n",
    "\n",
    "# Read and merge CSVs\n",
    "subregion_combined_df = read_and_merge_csvs(subregion_dir, subregion_cols)\n",
    "ba_combined_df = read_and_merge_csvs(ba_dir, ba_cols)\n",
    "\n",
    "# Before the loop, initialize an empty list to collect new rows\n",
    "new_rows = []\n",
    "\n",
    "# Iterate over BA combined dataframe to check for missing Balancing Authorities in Subregion dataframe\n",
    "for index, row in ba_combined_df.iterrows():\n",
    "    matching_rows = subregion_combined_df[(subregion_combined_df['UTC Time at End of Hour'] == row['UTC Time at End of Hour']) & \n",
    "                                           (subregion_combined_df['Balancing Authority'] == row['Balancing Authority'])]\n",
    "    if matching_rows.empty:\n",
    "        # If no matching row, prepare the new row with adjustments\n",
    "        new_row = row.to_dict()\n",
    "        new_row['Sub-Region'] = new_row['Balancing Authority']  # Set Sub-Region to Balancing Authority value\n",
    "        new_rows.append(new_row)\n",
    "\n",
    "# After the loop, add all new rows to the dataframe at once\n",
    "if new_rows:  # Check if there are any new rows to add\n",
    "    subregion_combined_df = pd.concat([subregion_combined_df, pd.DataFrame(new_rows)], ignore_index=True)\n",
    "\n",
    "# You may now use subregion_combined_df with the added records.\n",
    "# Optionally, you can save this dataframe to a new CSV file\n",
    "subregion_combined_df.to_csv(data_and_output_dir+'output/merged_subregion_with_ba.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the shapefile:\n",
      "Index(['STATEFP', 'COUNTYFP', 'COUNTYNS', 'AFFGEOID', 'GEOID', 'NAME', 'LSAD',\n",
      "       'ALAND', 'AWATER', 'geometry'],\n",
      "      dtype='object')\n",
      "\n",
      "Columns in the CSV file:\n",
      "Index(['Year', 'State_FIPS', 'State_Name', 'County_FIPS', 'County_Name',\n",
      "       'BA_Number', 'BA_Code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming data_and_output_dir is defined and contains the path to your data directory\n",
    "data_and_output_dir = '/Users/ansonkong/Downloads/Data for nyu work/' # Update this path\n",
    "\n",
    "# Path to the shapefile and CSV\n",
    "shapefile_path = data_and_output_dir + 'Input/cb_2018_us_county_500k/cb_2018_us_county_500k.shp'\n",
    "csv_path = data_and_output_dir + 'Input/ba_service_territory_2020.csv'\n",
    "\n",
    "# Read the shapefile using GeoPandas\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Read the CSV file. Use GeoPandas if you expect to work with geographic data in the CSV.\n",
    "# If the CSV doesn't contain geographic data, you could just use pandas with pd.read_csv(csv_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Print all columns from the shapefile\n",
    "print(\"Columns in the shapefile:\")\n",
    "print(gdf.columns)\n",
    "\n",
    "# Print all columns from the CSV file\n",
    "print(\"\\nColumns in the CSV file:\")\n",
    "print(df.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
