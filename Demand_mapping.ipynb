{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part of code, the goal is to go from mapping and county level weather data to to subgroup data\n",
    "# Define paths\n",
    "data_and_output_dir = '/Users/ansonkong/Downloads/Data for nyu work/'\n",
    "full_future_data_path = '/Users/ansonkong/Downloads/rcp85hotter/'\n",
    "full_historical_data_path = '/Users/ansonkong/Downloads/historic/'\n",
    "\n",
    "#this just smooth out operation\n",
    "cutoff_year=2020 #The year prediction dataset start\n",
    "start_year=2000 #The year historical \n",
    "end_year=2100\n",
    "\n",
    "#Debug mode(only output once)\n",
    "debug=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_7172/2308327632.py:17: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_7172/2308327632.py:17: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_7172/2308327632.py:17: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_7172/2308327632.py:17: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
      "/var/folders/8v/68yqll_54lzgb57b5q8_fbrc0000gn/T/ipykernel_7172/2308327632.py:17: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n"
     ]
    }
   ],
   "source": [
    "#import require file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "import os\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "ba_dir = data_and_output_dir+'Input/EIA 930/BA/'\n",
    "\n",
    "\n",
    "# Function to read and merge CSVs from a directory\n",
    "def read_and_merge_csvs(directory, cols):\n",
    "    combined_df = pd.DataFrame()\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith('.csv'):\n",
    "            df = pd.read_csv(os.path.join(directory, file), usecols=cols)\n",
    "            combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "ba_cols = ['UTC Time at End of Hour', 'Balancing Authority', 'Demand (MW)']\n",
    "ba_combined_df = read_and_merge_csvs(ba_dir, ba_cols)\n",
    "\n",
    "\n",
    "\n",
    "#Path, this will update itself\n",
    "mapping_path = data_and_output_dir + 'output/merged_rb_control_area_mapping.csv'\n",
    "future_population_data_path = data_and_output_dir + 'input/Electric_Retail_Service_Territories/ssp3_county_population.csv'\n",
    "historical_population_data_path = data_and_output_dir + 'input/county_populations_2000_to_2020.csv'\n",
    "output_path = data_and_output_dir + 'output/'\n",
    "\n",
    "# Load mapping and population data\n",
    "mapping_df = pd.read_csv(mapping_path)\n",
    "future_population_df = pd.read_csv(future_population_data_path)\n",
    "historical_population_df = pd.read_csv(historical_population_data_path)\n",
    "\n",
    "#crate looping for interp\n",
    "years = np.arange(cutoff_year, end_year + 1, 10)\n",
    "interp_years = np.arange(cutoff_year, end_year + 1)\n",
    "\n",
    "# Ensure FIPS codes are five zero-padded strings\n",
    "historical_population_df['FIPS'] = historical_population_df['county_FIPS'].astype(str).str.zfill(5)\n",
    "future_population_df['FIPS'] = future_population_df['FIPS'].astype(str).str.zfill(5)\n",
    "mapping_df['GEOID'] = mapping_df['GEOID'].astype(str).str.zfill(5)\n",
    "\n",
    "# Rename 'pop_{year}' columns to just '{year}'\n",
    "for year in range(start_year, cutoff_year+1):\n",
    "    if f'pop_{year}' in historical_population_df.columns:\n",
    "        historical_population_df.rename(columns={f'pop_{year}': str(year)}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming 'FIPS' is the column to join on and 'Year' is the column indicating the year in future_population_df\n",
    "for index, row in future_population_df.iterrows():\n",
    "    pop_values = [row[str(year)] for year in years if str(year) in row]\n",
    "    interpolator = interp1d(years, pop_values, kind='linear', fill_value=\"extrapolate\")\n",
    "    interpolated_values = interpolator(interp_years)\n",
    "    for year in interp_years:\n",
    "        future_population_df.at[index, str(year)] = interpolated_values[year - cutoff_year]\n",
    "\n",
    "\n",
    "# Select relevant columns for historical data\n",
    "historical_population_df = historical_population_df[['FIPS'] + [str(year) for year in range(start_year, cutoff_year)]]\n",
    "\n",
    "# Select relevant columns for future data\n",
    "future_population_df = future_population_df[['FIPS'] + [str(year) for year in interp_years]]\n",
    "\n",
    "# Concatenate historical and future dataframes\n",
    "combined_population_df = pd.merge(historical_population_df, future_population_df, on='FIPS', how='outer')\n",
    "# Merge combined population data with mapping data\n",
    "# Assuming 'GEOID' in mapping_df corresponds to 'FIPS' in population data\n",
    "combined_df = pd.merge(mapping_df, combined_population_df, left_on='GEOID', right_on='FIPS', how='inner')\n",
    "combined_df['FIPS']=combined_df['GEOID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['FIPS']=combined_df['FIPS_x']\n",
    "combined_df=combined_df[['FIPS','rb','geometry']+[str(year) for year in range(start_year, end_year+1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 'rb' values from 'p1' to 'p134' are present in combined_df.\n"
     ]
    }
   ],
   "source": [
    "# Generate the expected set of 'rb' values ('p1' to 'p134')\n",
    "expected_rbs = {'p' + str(i) for i in range(1, 135)}\n",
    "\n",
    "# Extract the unique 'rb' values from combined_df\n",
    "actual_rbs = set(combined_df['rb'].unique())\n",
    "\n",
    "# Check if combined_df['rb'] contains all values from 'p1' to 'p134'\n",
    "missing_rbs = expected_rbs - actual_rbs\n",
    "extra_rbs = actual_rbs - expected_rbs\n",
    "\n",
    "# Display the results\n",
    "if not missing_rbs:\n",
    "    print(\"All 'rb' values from 'p1' to 'p134' are present in combined_df.\")\n",
    "else:\n",
    "    print(f\"Missing 'rb' values in combined_df: {missing_rbs}\")\n",
    "\n",
    "if extra_rbs:\n",
    "    print(f\"Extra 'rb' values in combined_df not in the range 'p1' to 'p134': {extra_rbs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rb</th>\n",
       "      <th>geometry</th>\n",
       "      <th>BA_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>p1</td>\n",
       "      <td>MULTIPOLYGON (((-13687924.23979998 5965691.031...</td>\n",
       "      <td>BPAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p10</td>\n",
       "      <td>MULTIPOLYGON (((-13179969.92729995 3871459.308...</td>\n",
       "      <td>CISO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p100</td>\n",
       "      <td>MULTIPOLYGON (((-8663603.907299994 4741636.251...</td>\n",
       "      <td>PJM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p101</td>\n",
       "      <td>MULTIPOLYGON (((-9090644.388599997 2824968.523...</td>\n",
       "      <td>SEC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p102</td>\n",
       "      <td>MULTIPOLYGON (((-8924334.972400004 2980193.438...</td>\n",
       "      <td>FPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>p95</td>\n",
       "      <td>MULTIPOLYGON (((-9114960.995699998 4188937.632...</td>\n",
       "      <td>DUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>p96</td>\n",
       "      <td>MULTIPOLYGON (((-8956515.669899996 3812869.837...</td>\n",
       "      <td>SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>p97</td>\n",
       "      <td>MULTIPOLYGON (((-8873816.772200003 4375448.743...</td>\n",
       "      <td>DUK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>p98</td>\n",
       "      <td>MULTIPOLYGON (((-8678230.449399997 4014627.318...</td>\n",
       "      <td>CPLE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>p99</td>\n",
       "      <td>MULTIPOLYGON (((-8642814.420100002 4765990.366...</td>\n",
       "      <td>PJM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rb                                           geometry BA_Code\n",
       "0      p1  MULTIPOLYGON (((-13687924.23979998 5965691.031...    BPAT\n",
       "1     p10  MULTIPOLYGON (((-13179969.92729995 3871459.308...    CISO\n",
       "2    p100  MULTIPOLYGON (((-8663603.907299994 4741636.251...     PJM\n",
       "3    p101  MULTIPOLYGON (((-9090644.388599997 2824968.523...     SEC\n",
       "4    p102  MULTIPOLYGON (((-8924334.972400004 2980193.438...     FPL\n",
       "..    ...                                                ...     ...\n",
       "129   p95  MULTIPOLYGON (((-9114960.995699998 4188937.632...     DUK\n",
       "130   p96  MULTIPOLYGON (((-8956515.669899996 3812869.837...      SC\n",
       "131   p97  MULTIPOLYGON (((-8873816.772200003 4375448.743...     DUK\n",
       "132   p98  MULTIPOLYGON (((-8678230.449399997 4014627.318...    CPLE\n",
       "133   p99  MULTIPOLYGON (((-8642814.420100002 4765990.366...     PJM\n",
       "\n",
       "[134 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb_to_ba=pd.read_csv(data_and_output_dir + 'output/rb_to_ba.csv')\n",
    "rb_to_ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BPAT', 'CISO', 'PJM', 'SEC', 'FPL', 'MISO', 'NEVP', 'NYIS',\n",
       "       'ISNE', 'WALC', 'AVA', 'IPCO', 'PACE', 'NWMT', 'SWPP', 'WACM',\n",
       "       'AZPS', 'PNM', 'PSCO', 'AECI', 'EPE', 'PACW', 'ERCO', 'TVA',\n",
       "       'SOCO', 'AEC', 'DUK', 'SC', 'CPLE'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb_to_ba['BA_Code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_with_county_and_rb_df = pd.merge(combined_df, rb_to_ba, on='rb', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 'rb' values from 'p1' to 'p134' are present in combined_df.\n"
     ]
    }
   ],
   "source": [
    "# Extract the unique 'rb' values from combined_df\n",
    "actual_rbs = set(population_with_county_and_rb_df ['rb'].unique())\n",
    "\n",
    "# Check if combined_df['rb'] contains all values from 'p1' to 'p134'\n",
    "missing_rbs = expected_rbs - actual_rbs\n",
    "extra_rbs = actual_rbs - expected_rbs\n",
    "\n",
    "# Display the results\n",
    "if not missing_rbs:\n",
    "    print(\"All 'rb' values from 'p1' to 'p134' are present in combined_df.\")\n",
    "else:\n",
    "    print(f\"Missing 'rb' values in combined_df: {missing_rbs}\")\n",
    "\n",
    "if extra_rbs:\n",
    "    print(f\"Extra 'rb' values in combined_df not in the range 'p1' to 'p134': {extra_rbs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the year columns to sum up\n",
    "year_columns = [str(year) for year in range(start_year, end_year + 1)]\n",
    "\n",
    "# Group by 'rb' and 'BA_Code', then sum up the specified year columns\n",
    "aggregated_df = population_with_county_and_rb_df.groupby(['rb', 'BA_Code'])[year_columns].sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rb BA_Code      2000      2001      2002      2003      2004      2005  \\\n",
      "0    p1    BPAT  0.450298  0.450676  0.449738  0.448958  0.448836  0.448224   \n",
      "1   p10    CISO  0.520191  0.520019  0.521367  0.522828  0.524362  0.524909   \n",
      "2  p100     PJM  0.003103  0.003143  0.003189  0.003227  0.003270  0.003331   \n",
      "3  p101     SEC  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "4  p102     FPL  1.000000  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "\n",
      "       2006      2007  ...      2091      2092      2093      2094      2095  \\\n",
      "0  0.448384  0.447617  ...  0.455866  0.456058  0.456253  0.456450  0.456651   \n",
      "1  0.524787  0.523626  ...  0.553351  0.553587  0.553828  0.554073  0.554323   \n",
      "2  0.003387  0.003424  ...  0.003885  0.003872  0.003860  0.003847  0.003834   \n",
      "3  1.000000  1.000000  ...  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "4  1.000000  1.000000  ...  1.000000  1.000000  1.000000  1.000000  1.000000   \n",
      "\n",
      "       2096      2097      2098      2099      2100  \n",
      "0  0.456856  0.457064  0.457275  0.457490  0.457708  \n",
      "1  0.554578  0.554839  0.555104  0.555376  0.555653  \n",
      "2  0.003821  0.003807  0.003793  0.003779  0.003765  \n",
      "3  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
      "4  1.000000  1.000000  1.000000  1.000000  1.000000  \n",
      "\n",
      "[5 rows x 103 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loop through each year column to perform the transformation\n",
    "for year in year_columns:\n",
    "    # Calculate the sum for each 'rb' group in the current year column\n",
    "    rb_sum = aggregated_df.groupby('BA_Code')[year].transform('sum')\n",
    "    \n",
    "    # Divide each value by its corresponding 'rb' total for the current year\n",
    "    aggregated_df[year] = aggregated_df[year] / rb_sum\n",
    "\n",
    "# Display the first few rows of the transformed DataFrame to verify the results\n",
    "print(aggregated_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 'rb' values from 'p1' to 'p134' are present in combined_df.\n"
     ]
    }
   ],
   "source": [
    "# Extract the unique 'rb' values from combined_df\n",
    "actual_rbs = set(aggregated_df['rb'].unique())\n",
    "\n",
    "# Check if combined_df['rb'] contains all values from 'p1' to 'p134'\n",
    "missing_rbs = expected_rbs - actual_rbs\n",
    "extra_rbs = actual_rbs - expected_rbs\n",
    "\n",
    "# Display the results\n",
    "if not missing_rbs:\n",
    "    print(\"All 'rb' values from 'p1' to 'p134' are present in combined_df.\")\n",
    "else:\n",
    "    print(f\"Missing 'rb' values in combined_df: {missing_rbs}\")\n",
    "\n",
    "if extra_rbs:\n",
    "    print(f\"Extra 'rb' values in combined_df not in the range 'p1' to 'p134': {extra_rbs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balancing Authority</th>\n",
       "      <th>UTC Time at End of Hour</th>\n",
       "      <th>Demand (MW)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AEC</td>\n",
       "      <td>01/01/2018 7:00:00 AM</td>\n",
       "      <td>748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AEC</td>\n",
       "      <td>01/01/2018 8:00:00 AM</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AEC</td>\n",
       "      <td>01/01/2018 9:00:00 AM</td>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AEC</td>\n",
       "      <td>01/01/2018 10:00:00 AM</td>\n",
       "      <td>814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AEC</td>\n",
       "      <td>01/01/2018 11:00:00 AM</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158641</th>\n",
       "      <td>YAD</td>\n",
       "      <td>01/01/2019 1:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158642</th>\n",
       "      <td>YAD</td>\n",
       "      <td>01/01/2019 2:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158643</th>\n",
       "      <td>YAD</td>\n",
       "      <td>01/01/2019 3:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158644</th>\n",
       "      <td>YAD</td>\n",
       "      <td>01/01/2019 4:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3158645</th>\n",
       "      <td>YAD</td>\n",
       "      <td>01/01/2019 5:00:00 AM</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3158646 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Balancing Authority UTC Time at End of Hour Demand (MW)\n",
       "0                       AEC   01/01/2018 7:00:00 AM         748\n",
       "1                       AEC   01/01/2018 8:00:00 AM         763\n",
       "2                       AEC   01/01/2018 9:00:00 AM         789\n",
       "3                       AEC  01/01/2018 10:00:00 AM         814\n",
       "4                       AEC  01/01/2018 11:00:00 AM         848\n",
       "...                     ...                     ...         ...\n",
       "3158641                 YAD   01/01/2019 1:00:00 AM         NaN\n",
       "3158642                 YAD   01/01/2019 2:00:00 AM         NaN\n",
       "3158643                 YAD   01/01/2019 3:00:00 AM         NaN\n",
       "3158644                 YAD   01/01/2019 4:00:00 AM         NaN\n",
       "3158645                 YAD   01/01/2019 5:00:00 AM         NaN\n",
       "\n",
       "[3158646 rows x 3 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ba_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codes unique to ba_combined_df: {'AVRN', 'SPA', 'GLHB', 'NSB', 'FMPP', 'HGMA', 'GRID', 'GRMA', 'DOPD', 'GVL', 'WWA', 'TAL', 'WAUW', 'IID', 'DEAA', 'SCL', 'TEPC', 'LGEE', 'GWA', 'SEPA', 'YAD', 'GCPD', 'EEI', 'JEA', 'PSEI', 'CPLW', 'FPC', 'SRP', 'LDWP', 'TIDC', 'OVEC', 'WAUE', 'CHPD', 'TEC', 'BANC', 'PGE', 'SCEG', 'TPWR', 'GRIF', 'HST'}\n",
      "Codes unique to rb_to_ba: set()\n",
      "Perfect matches: {'SWPP', 'SOCO', 'SC', 'PNM', 'FPL', 'IPCO', 'AEC', 'CPLE', 'AZPS', 'ERCO', 'MISO', 'PJM', 'TVA', 'BPAT', 'NWMT', 'EPE', 'NYIS', 'PACW', 'AVA', 'WALC', 'CISO', 'AECI', 'DUK', 'WACM', 'PACE', 'SEC', 'ISNE', 'NEVP', 'PSCO'}\n"
     ]
    }
   ],
   "source": [
    "# Assuming ba_combined_df and rb_to_ba are your DataFrames\n",
    "\n",
    "# Extract unique 'Balancing Authority' and 'BA_Code' values\n",
    "unique_ba_combined = set(ba_combined_df['Balancing Authority'].unique())\n",
    "unique_rb_to_ba = set(rb_to_ba['BA_Code'].unique())\n",
    "\n",
    "# Find codes that are unique to ba_combined_df\n",
    "unique_to_ba_combined = unique_ba_combined - unique_rb_to_ba\n",
    "\n",
    "# Find codes that are unique to rb_to_ba\n",
    "unique_to_rb_to_ba = unique_rb_to_ba - unique_ba_combined\n",
    "\n",
    "# Find codes that are present in both\n",
    "perfect_matches = unique_ba_combined.intersection(unique_rb_to_ba)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Codes unique to ba_combined_df: {unique_to_ba_combined}\")\n",
    "print(f\"Codes unique to rb_to_ba: {unique_to_rb_to_ba}\")\n",
    "print(f\"Perfect matches: {perfect_matches}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NBSO', 'AEC', 'AMPL', 'AZPS', 'AECI', 'BPAT', 'CISO', 'CPLE',\n",
       "       'CHPD', 'CEA', 'DOPD', 'DUK', 'EPE', 'ERCO', 'FPL', 'FPC', 'GVL',\n",
       "       'HST', 'IPCO', 'IID', 'JEA', 'LDWP', 'LGEE', 'NWMT', 'NEVP',\n",
       "       'ISNE', 'NSB', 'NYIS', 'OVEC', 'PACW', 'PACE', 'FMPP', 'GCPD',\n",
       "       'PJM', 'PGE', 'PSCO', 'PNM', 'PSEI', 'BANC', 'SRP', 'SCL', 'SCEG',\n",
       "       'SC', 'SPA', 'SOCO', 'TPWR', 'TAL', 'TEC', 'TVA', 'TIDC', 'HECO',\n",
       "       'WAUW', 'AVA', 'SEC', 'TEPC', 'WALC', 'WACM', 'SEPA', 'MISO',\n",
       "       'SWPP'], dtype=object)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = data_and_output_dir + 'Input/ba_service_territory_2020.csv'\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "df['BA_Code'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
